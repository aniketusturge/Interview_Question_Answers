

Explain how container orchestration works and why it's important.

Container orchestration is the automated management of containerized applications at scale â€” across multiple hosts or environments. It handles the deployment, scheduling, scaling, networking, health monitoring, and lifecycle management of containers.

ğŸ”§ How It Works:
A container orchestrator (like Kubernetes, Docker Swarm, or Amazon ECS) coordinates and automates the following:
1.Deployment: Automatically places containers on the best-fit nodes in a cluster.
2.Scaling: Dynamically adjusts the number of containers based on load or policies.
3.Load Balancing: Distributes traffic across containers to ensure high availability.
4.Self-Healing: Automatically restarts failed containers, replaces unresponsive ones, and maintains the desired state.
5.Service Discovery: Assigns services DNS names and abstracts internal IPs.
6.Rolling Updates & Rollbacks: Enables zero-downtime deployments with built-in rollback support.
7.Resource Management: Optimizes CPU and memory usage across containers.

ğŸš€ Why Itâ€™s Important:
1.Scalability: Orchestration enables you to run and scale thousands of containers efficiently.
2.Resilience: Ensures high availability and fault tolerance through self-healing mechanisms.
3.Automation: Reduces manual overhead in managing complex microservices architectures.
4.Consistency: Provides repeatable deployments across dev, test, and prod.
5.DevOps Alignment: Enables CI/CD integration, GitOps workflows, and modern infrastructure-as-code practices.

ğŸ” Where Iâ€™ve Used It:
In my recent projects, Iâ€™ve used Kubernetes for orchestrating microservices deployed on AWS EKS. It managed 20+ services across dev, staging, and production clusters with autoscaling, resource quotas, and Helm-based deployments â€” all tied into a Jenkins CI/CD pipeline.

In summary, container orchestration is essential for running modern cloud-native applications efficiently, reliably, and at scale. It abstracts the operational complexity so teams can focus on delivering business value.


----------------------------------------------------------------------


What is a CrashLoopBackOff error in k8's, and how do you resolve it?

CrashLoopBackOff in Kubernetes means a container in a Pod is repeatedly failing to start, Kubernetes restarts it, and it crashes again, entering a loop.
The â€œBackOffâ€ part means Kubernetes is waiting progressively longer before restarting the container to avoid hammering the system.

Common Causes
1.Application errors â€“ misconfigured environment variables, bad code, missing files.
2.Dependency issues â€“ the app tries to connect to a DB/service that isnâ€™t ready yet.
3.Configuration errors â€“ incorrect command/entrypoint in the container spec.
4.Resource limits â€“ container killed due to OOM (out of memory) or CPU throttling.
5.Image/permission issues â€“ wrong Docker image, missing permissions, or secrets.

How to Resolve
1.Check pod status and events
bash
kubectl describe pod <pod-name>
Look for messages about restarts, failed probes, or image pull errors.

2.View container logs
bash
kubectl logs <pod-name> --previous
--previous shows logs from the last crashed container instance.

3.Debug the configuration
-Verify env vars, config maps, and secrets.
-Ensure command and args in the manifest are correct.

4.Check readiness & liveness probes
-Misconfigured probes can kill a healthy pod.

5.Check resource limits
If OOMKilled:

yaml
resources:
  limits:
    memory: 512Mi
Increase as needed.

6.Test dependencies
Make sure services/databases the app needs are running and reachable.

ğŸ’¡ Interview add-on

"In short, CrashLoopBackOff is usually an application or configuration issue, not a Kubernetes bug. My approach is to check pod events, logs, and resource configs, then test dependencies to fix the root cause. In some cases, I deploy a temporary debug container or use kubectl exec to inspect the container filesystem and environment directly."

----------------------------------------

How do you validate whether a deployment manifest file in k8's has errors?

Hereâ€™s how you can validate a Kubernetes deployment manifest before applying it â€” both for syntax and cluster compatibility:

1. Local YAML Syntax Validation
Before even talking to Kubernetes:

bash
yamllint deployment.yaml
Ensures indentation, key formatting, and YAML structure are correct.

Catch simple syntax errors early.

2. Dry Run with Kubernetes API (Syntax + API Schema Validation)
bash
kubectl apply --dry-run=client -f deployment.yaml
-Checks manifest syntax locally against Kubernetes schema without sending it to the cluster.
-Fast and safe.

3. Server-side Dry Run (Full Validation Against Cluster)
bash
kubectl apply --dry-run=server -f deployment.yaml
-Validates against the actual API server and cluster version.
-Catches errors like unsupported API versions, missing namespaces, or wrong field names.

4. Use kubectl diff to Preview Changes
bash
kubectl diff -f deployment.yaml
-Shows what changes will happen if applied.
-Helps avoid overwriting configs unintentionally.

5. CI/CD Manifest Validation
Integrate tools like kubeval, kubeconform, or OPA Gatekeeper in pipelines.
Example with kubeval:

bash

kubeval deployment.yaml
Ensures manifests follow Kubernetes schemas and your orgâ€™s policy rules.

ğŸ’¡ Interview tip answer
"Before applying a manifest, I first run YAML linting, then use kubectl apply --dry-run=server to validate it against the cluster. If it passes, I might use kubectl diff to see intended changes. For automation, I integrate kubeval/kubeconform in CI pipelines to ensure manifests are correct before merging."

-------------------------------------------------------

How do you schedule a k8's deployment to run on a specific node?
You can schedule a Kubernetes deployment to run on a specific node using node selectors, node affinity, or taints & tolerations â€” depending on how strict you want the scheduling to be.

1.  or (Simple & Direct)
Add a label to the node:

bash
kubectl label nodes <node-name> disktype=ssd

In your Deployment manifest:
yaml
spec:
  template:
    spec:
      nodeSelector:
        disktype: ssd
Pods will only schedule on nodes with that label.

2. Node Affinity (More Flexible)
Offers operators like In, NotIn, Exists for matching labels.

Example:
yaml
spec:
  template:
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: disktype
                operator: In
                values:
                - ssd
Can use preferredDuringSchedulingIgnoredDuringExecution for soft rules.

3. Taints & Tolerations (Restrictive Control)
Taint the node so only specific pods can run there:

bash
kubectl taint nodes <node-name> special=true:NoSchedule

Add toleration in manifest:
yaml
spec:
  template:
    spec:
      tolerations:
      - key: "special"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
Useful for isolating workloads (e.g., production-only node).

ğŸ’¡ Interview-ready summary

"To schedule a pod on a specific node, I can use nodeSelector for a quick match, nodeAffinity for flexible rules, or taints and tolerations for strict control. My usual approach is to label the target node, then reference that label in the deployment manifest, ensuring the pod only runs on that node."


-------------------------------------------------

What are taints, tolerations, and node affinity in Kubernetes?

Hereâ€™s a clear, interview-focused explanation of taints, tolerations, and node affinity in Kubernetes â€” plus how they differ.

1. Taints
Definition: A taint is applied to a node to repel certain pods from scheduling there unless those pods explicitly tolerate the taint.
Purpose: Prevent unwanted workloads from running on a node (isolate workloads, reserve nodes for special purposes).
Command to add taint:
   kubectl taint nodes <node-name> key=value:NoSchedule

Effects:
1.NoSchedule â†’ Pod wonâ€™t be scheduled unless it tolerates the taint.
2.PreferNoSchedule â†’ Scheduler tries to avoid the node but may still place pods.
3.NoExecute â†’ Evicts existing pods that donâ€™t tolerate the taint.

2. Tolerations
Definition: A toleration is set on a pod, allowing it to be scheduled onto nodes with matching taints.
Purpose: Let specific pods run on tainted nodes.
Example in manifest:

yaml
spec:
  tolerations:
  - key: "key"
    operator: "Equal"
    value: "value"
    effect: "NoSchedule"

3. Node Affinity
Definition: Rules that tell the scheduler to prefer or require a pod to run on nodes with specific labels.
Purpose: Attract pods to certain nodes (opposite of taints).
Types:
requiredDuringSchedulingIgnoredDuringExecution â†’ Hard rule (must match).
preferredDuringSchedulingIgnoredDuringExecution â†’ Soft rule (best effort).

Example in manifest:

yaml
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd

Quick Comparison Table
Feature		 Applied To		Purpose				Behavior
Taint		 Node		Repels pods unless tolerated		Blocks pods by default
Toleration	 Pod		Allows scheduling on tainted nodes	Lets pod bypass block
Node Affinity	 Pod		Attracts pods to specific nodes		Scheduler prefers/requires

ğŸ’¡ Interview-ready summary

"Taints are like a 'keep out' sign on nodes, tolerations are a podâ€™s permission to enter, and node affinity is more like a 'preferred location' sign for pods. I use taints/tolerations for isolation and affinity for placement control."

----------------------------------------------------

What are PV and PVC in k8's?

Persistent Volume (PV)
-Definition: A PV is a piece of storage in a Kubernetes cluster that has been provisioned by an administrator or dynamically via a StorageClass.
-Lifecycle: Exists independently of pods; data remains even if pods are deleted.
-Analogy: Think of it as the â€œactual storage deviceâ€ â€” could be AWS EBS, Azure Disk, NFS, etc.

Example PV YAML:

yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-example
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /data/pv1

Persistent Volume Claim (PVC)
-Definition: A PVC is a request for storage by a pod.
-Usage: Defines size, access modes, and sometimes StorageClass. Kubernetes finds a suitable PV that meets the claim.
-Analogy: Think of it as a â€œrequest formâ€ asking for storage.

Example PVC YAML:

yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-example
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi

Relationship
-PVCs bind to PVs that match their request.
-Pods use PVCs, not PVs directly.
-If PVC is deleted (depending on the PVâ€™s reclaimPolicy), the PV can:
  Retain â†’ keep the data.
  Delete â†’ delete the storage.
  Recycle â†’ wipe and make available again.

ğŸ’¡ Interview-ready one-liner

"A PV is the actual storage resource in Kubernetes, and a PVC is a request for that storage. Pods use PVCs to get storage without worrying about the underlying storage type."

----------------------------------------------------------

How do you monitor PVC storage usage, and what actions do you take if itâ€™s full?

1. How to Monitor PVC Storage Usage
PVC usage is not directly exposed in kubectl get pvc â€” you need to check underlying volume metrics.

Approaches:
1.Using kubectl & underlying node

bash
kubectl describe pvc <pvc-name>
â†’ Shows requested vs capacity (but not live usage).
Then check the underlying volume on the node or storage provider (e.g., AWS EBS metrics in CloudWatch).

2.Metrics Server + Prometheus
-Install Prometheus and configure it to scrape kubeletâ€™s /metrics for volume stats.

3.Query metrics like:

nginx
kubelet_volume_stats_used_bytes
kubelet_volume_stats_capacity_bytes
Visualize in Grafana with alerts.

4.Cloud Provider Monitoring
AWS â†’ CloudWatch metrics for EBS volumes.
Azure â†’ Azure Monitor.
GCP â†’ Cloud Monitoring.

2. What Actions to Take if PVC is Full
-Short-Term Mitigation
-Identify and delete unnecessary data/logs in the volume.
-Increase PVC size if the storage class allows expansion:

yaml
resources:
  requests:
    storage: 20Gi
Then:

bash
kubectl apply -f pvc.yaml

Long-Term Prevention
1.Enable StorageClass with allowVolumeExpansion: true.
2.Set up alerts in Prometheus/Grafana for >80% usage.
3.Use log rotation and retention policies for apps writing to PVC.
4.Implement backup & archive policies to move old data off PVC.

If possible, switch to a dynamically expandable backend (like EBS, CSI drivers).

ğŸ’¡ Interview-ready one-liner
"I monitor PVC usage via Prometheus metrics like kubelet_volume_stats_used_bytes and cloud provider dashboards. If itâ€™s full, I first clean unnecessary data, then expand the PVC if supported, and set alerts to prevent recurrence."


-----------------------------------------------

What checks would you perform if pod has an ImagePullBackOff error?

Meaning
ImagePullBackOff means Kubernetes tried to pull the container image from the registry, failed, and is now backing off before retrying.

Checks & Troubleshooting Steps
1. Check Pod Events for Error Details
kubectl describe pod <pod-name>
Look under Events for messages like:
  -ErrImagePull
  -unauthorized: authentication required
  -manifest not found

2. Verify Image Name & Tag
-Ensure image: <registry>/<repo>:<tag> is correct.
-Check for typos or missing tags.
-Try pulling manually from your machine:

bash
docker pull <image>

3. Check Image Availability in Registry
-Make sure the image exists and is pushed to the correct repository.
-If itâ€™s a private registry, confirm credentials are set.

4. Validate ImagePullSecrets
For private registries:

bash
kubectl get secret <secret-name> --namespace=<ns>
Ensure the secret is referenced in the pod/deployment:

yaml
spec:
  imagePullSecrets:
  - name: myregistry-secret

5. Check Network/DNS Access
Pod must reach the registry endpoint.

Test with:
bash
kubectl exec -it <pod> -- nslookup <registry>

6. Verify Node Has Access
-If using self-hosted nodes, confirm Docker/container runtime can access the registry.
-For EKS/GKE/AKS, ensure IAM/permissions are correct if pulling from cloud-native registries.

ğŸ’¡ Interview-ready one-liner
"If a pod is in ImagePullBackOff, I check pod events with kubectl describe, verify the image name and tag, confirm the image exists in the registry, ensure imagePullSecrets are set for private repos, and check network access from the node to the registry."

------------------------------------------------------

How do you use Kubernetes probes (liveness/readiness)? Why are they important?
In Kubernetes, I use liveness and readiness probes to monitor the health and availability of containers. They are critical for ensuring zero-downtime deployments, self-healing, and graceful service discovery.

ğŸ” 1. What Are Probes and Why Are They Important?
Liveness Probe checks if the application is alive.
-If it fails repeatedly, Kubelet restarts the container.
-Prevents stale or stuck containers from running forever.

Readiness Probe checks if the application is ready to serve traffic.
-If it fails, the pod is removed from the service endpoints.
-Prevents users from hitting pods that are still starting up.

Both probes help Kubernetes manage resilience, zero-downtime rollouts, and autoscaling.

âš™ï¸ 2. How I Use Them (Examples)
ğŸ§  HTTP-Based Probes
Used for web apps exposing health endpoints.

yaml
livenessProbe:
  httpGet:
    path: /healthz
    port: 8080
  initialDelaySeconds: 10
  periodSeconds: 10
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /ready
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 5

ğŸ§  Command-Based Probes
Used for apps that canâ€™t expose HTTP endpoints.
yaml
livenessProbe:
  exec:
    command:
      - cat
      - /tmp/healthy
  periodSeconds: 10
ğŸ’¡ 3. Best Practices
-Always use readiness during rolling deploymentsâ€”especially for apps with delayed startup like Java or Node.js.
-Use liveness with cautionâ€”too aggressive settings can cause restart loops.
-Separate /healthz and /ready endpoints if your app has different logic for "running" vs. "ready".
-Combine with graceful shutdown logic to prevent in-flight request drops.

âœ… Summary (Wrap-Up):
In short, I use liveness probes to ensure containers are self-healing, and readiness probes to control traffic routing and avoid hitting unready pods. Together, they enhance app availability, auto-scaling efficiency, and smooth rolling deployments.


---------------------------------------------------------

Q. How do you do Helm-based deployments in Kubernetes?
-I use Helm to package, manage, and deploy Kubernetes applications efficiently. 
-Helm simplifies deployment by templatizing Kubernetes manifests and allows consistent, repeatable, and parameterized deployments across environments.

ğŸš€ 1. Helm Chart Basics
A Helm chart is a package that contains:
perl
my-app/
  Chart.yaml          # Metadata
  values.yaml         # Default configuration
  templates/          # Templated Kubernetes manifests
I either:
-Use official charts (e.g., bitnami/nginx)
-Or build custom charts for in-house applications

ğŸ§ª 2. Deploying with Helm
I deploy with:
helm install my-app ./my-app-chart -n my-namespace

To upgrade:
helm upgrade my-app ./my-app-chart -f values-prod.yaml

To test changes before applying:
helm template ./my-app-chart -f values-dev.yaml

ğŸ”„ 3. Managing Environments with values.yaml
Each environment has its own override file:

perl
values-dev.yaml
values-stage.yaml
values-prod.yaml
This keeps configuration DRY and scalable:

yaml
replicaCount: 2
image:
  repository: myrepo/my-app
  tag: v1.2.3
env:
  - name: ENV
    value: "dev"

ğŸ§© 4. CI/CD Integration (e.g., Jenkins, GitHub Actions)
In CI/CD:
I automate Helm deployments using:
helm upgrade --install my-app ./my-app-chart \
  -f values-${ENV}.yaml \
  --set image.tag=${BUILD_TAG} \
  --namespace ${K8S_NAMESPACE}
Integrated RBAC and Kubernetes service accounts for secure access.

ğŸ” 5. Security and Rollbacks
Helm supports versioned releases and rollback:
helm rollback my-app 1
I use this for quick recovery during failed upgrades.

âœ… Summary (Wrap-Up):
So, I use Helm for deploying apps in Kubernetes by creating reusable charts, managing configuration through values.yaml, and automating deployments through CI/CD. Helm makes it easy to scale, rollback, and maintain environment consistency.


---------------------------------------------------------

Q. Whatâ€™s the difference between StatefulSet and Deployment in Kubernetes?
The main difference between a StatefulSet and a Deployment in Kubernetes lies in how they manage pod identity, storage, and ordering. I choose between them based on whether the app is stateless or stateful.

ğŸ“¦ 1. Deployment â€“ For Stateless Applications
-Used for apps where each pod is identical (e.g., web servers, APIs).
-Pods are interchangeable â€“ no persistent identity.
-Ephemeral storage is fine (backed by emptyDir or shared volumes).
-Scaling and rolling updates are fast and unordered.

Example:
yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest

ğŸ’¾ 2. StatefulSet â€“ For Stateful Applications
-Used for apps that require stable network identity and persistent storage (e.g., databases, Kafka, RabbitMQ).
-Each pod has a unique, stable hostname: pod-0, pod-1, etc.
-Maintains ordering and uniqueness during scaling and updates.
-Uses persistent volume claims (PVCs) per pod â€” PVCs are not deleted when pods are removed.

Example:
yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  serviceName: "mysql"
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:5.7
        volumeMounts:
        - name: mysql-data
          mountPath: /var/lib/mysql
  volumeClaimTemplates:
  - metadata:
      name: mysql-data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 1Gi
ğŸ” 3. Key Differences Summary Table
Feature			Deployment			StatefulSet
Pod identity		Anonymous/interchangeable	Unique (pod-0, pod-1, etc.)
Use case		Stateless apps			Stateful apps (DBs, message queues)
Storage			Shared or ephemeral		Dedicated persistent storage per pod
Pod ordering		No				Enforced (start, scale, update, delete)
Headless service	Optional			Required (for stable DNS)
Volume reuse		Not guaranteed			PVCs tied to pod identity

âœ… Summary (Wrap-Up):
So, I use Deployments for scalable, stateless services, and StatefulSets when the application needs stable identity, persistent storage, and ordered deployment â€” such as databases or clustered apps like Kafka and Cassandra.

---------------------------------------------------------


Q. How do you store and access persistent data inside a Kubernetes pod?
To store and access persistent data inside a Kubernetes pod, I use Persistent Volumes (PVs) and Persistent Volume Claims (PVCs). This decouples storage from the pod lifecycle, so data isn't lost when the pod restarts or gets rescheduled.

ğŸ“¦ 1. Why Not Use EmptyDir or Container Filesystem?
By default, pod data is stored in the container filesystem or emptyDir, which is:
Ephemeral
Lost when the pod dies or moves
For persistent workloads like databases or logs, I always use persistent volumes.

ğŸ› ï¸ 2. Steps to Use Persistent Storage in Kubernetes
Step 1: Create a Persistent Volume (PV)
(Or use a dynamic provisioner like EBS on AWS, Azure Disk, etc.)
yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-pv
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /mnt/data
In cloud environments, I typically use dynamic provisioning with a StorageClass.

Step 2: Create a Persistent Volume Claim (PVC)
yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
Step 3: Mount the PVC into the Pod
yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-container
    image: nginx
    volumeMounts:
    - mountPath: "/usr/share/nginx/html"
      name: my-storage
  volumes:
  - name: my-storage
    persistentVolumeClaim:
      claimName: my-pvc
â˜ï¸ 3. In Production (Cloud Storage)
In real-world environments:
Use dynamic provisioning via a StorageClass
AWS: EBS
Azure: Managed Disks
GCP: Persistent Disks

Example:
yaml
storageClassName: gp2
ğŸ§  Best Practices
-Use ReadWriteOnce, ReadOnlyMany, or ReadWriteMany based on access needs.
-Clean up unused PVCs to prevent orphaned cloud resources.
-In StatefulSets, PVCs are automatically created per pod with volumeClaimTemplates.

âœ… Summary (Wrap-Up):
I use Persistent Volume Claims (PVCs) to store persistent data in Kubernetes. This ensures data survives pod restarts or rescheduling. In cloud-native setups, I rely on dynamic provisioning with StorageClass to integrate with cloud block or file storage.


---------------------------------------------------------

Q. How do you enable RBAC in Kubernetes or IAM in AWS to limit access to resources?
I use Kubernetes RBAC and AWS IAM to enforce least-privilege access to cluster and cloud resources. Both systems allow defining who can access what, and I apply them in a structured, auditable, and secure way.

â˜¸ï¸ Kubernetes â€“ Enabling and Using RBAC
ğŸ” 1. Enable RBAC
RBAC is enabled by default in most managed Kubernetes services (EKS, AKS, GKE).
If using a self-managed cluster, ensure the API server includes:

ini
--authorization-mode=RBAC

ğŸ› ï¸ 2. Define Roles and Bindings
RBAC is used by creating Role / ClusterRole, and binding it with RoleBinding / ClusterRoleBinding.

âœ… Example: Read-only access to pods in dev namespace
yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: dev
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods-binding
  namespace: dev
subjects:
- kind: User
  name: dev-user@example.com
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io

ğŸ” 3. Best Practices
Use service accounts for automation (e.g., CI/CD pods).
Avoid ClusterRoleBinding unless necessary.
Regularly audit roles and bindings.
Combine with OPA Gatekeeper or Kyverno to enforce RBAC policies.

â˜ï¸ AWS â€“ Using IAM to Limit Access
ğŸ§± 1. Define IAM Policies
IAM in AWS uses policies (JSON) that define allowed/denied actions on resources.

âœ… Example: Read-only access to S3
json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": ["s3:GetObject"],
      "Resource": ["arn:aws:s3:::my-bucket/*"]
    }
  ]
}
Attach this policy to:
-IAM User (human access)
-IAM Role (EC2, Lambda, EKS nodes)
-IAM Group (for teams)

ğŸ” 2. Use IAM Roles with STS (Temporary Access)
Use IAM Roles + STS to issue temporary credentials with expiration.

Enforce role assumption in CI/CD and automation workflows.

ğŸ§  3. Fine-Grained Access Control
Use condition keys to restrict access by:
-IP address
-Tags
-Time of day
Leverage resource-based policies (e.g., for S3 buckets or Lambda functions).

âœ… Summary (Wrap-Up):
I enable and use Kubernetes RBAC to control access inside clusters, and AWS IAM to limit cloud resource access. I always apply the principle of least privilege, use role-based access, and automate audits to ensure secure, maintainable access control across the stack.

---------------------------------------------------

What are pods, deployments, and services in Kubernetes?
In Kubernetes, Pods, Deployments, and Services are foundational building blocks that help manage and scale containerized applications efficiently.

ğŸ”¹ Pod â€“ The Smallest Unit in Kubernetes
-A Pod is the smallest deployable unit in Kubernetes.
-It encapsulates one or more containers that share the same network, storage, and lifecycle.
-Containers in the same pod can communicate over localhost.

Analogy: Think of a Pod like a single room with tightly connected components (containers) working together.

ğŸ”¹ Deployment â€“ Declarative Management of Pods
A Deployment is a higher-level abstraction that manages a replica set of Pods.
It defines:
-The desired number of Pod replicas
-The container image and version
-Update strategies (rolling updates, rollbacks)
Kubernetes ensures the actual state matches the desired state defined in the Deployment YAML.

Use case: If a pod crashes or a node goes down, the Deployment controller automatically replaces the pod.

ğŸ”¹ Service â€“ Stable Access to Pods
-A Service is an abstraction layer that exposes a set of pods as a network service.
-Since Pods are ephemeral and IPs change, Services provide a stable endpoint using DNS and a load-balancing mechanism.

Types of Services:
1.ClusterIP: Internal-only access within the cluster
2.NodePort: Exposes the app on a static port on each node
3.LoadBalancer: Integrates with cloud load balancers (e.g., AWS ELB)

Analogy: Think of a Service as a receptionist who knows which pod to route your request to, even if the actual worker (pod) changes.

âœ… How They Work Together:
1.Deployment creates and manages multiple Pods.
2.A Service exposes those Pods to internal or external users.
3.Kubernetes ensures scaling, healing, and availability automatically.

In my experience, Iâ€™ve used this trio to deploy stateless microservices, expose them internally via ClusterIP during dev, and externally via LoadBalancer in production â€” all managed through Helm and GitOps pipelines.

--------------------------------------------------------------

How do you perform a rolling update in Kubernetes using a YAML file?
In Kubernetes, a rolling update is the default strategy used by Deployments to update pods gradually without downtime. It replaces old pods with new ones in a controlled manner.

To perform a rolling update using a YAML file, I simply update the image version (or any other pod spec) in the Deployment YAML and apply the change with kubectl apply. Kubernetes automatically handles the update incrementally based on the rollout strategy defined.

âœ… Example Deployment YAML (with Rolling Update Strategy):
yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-app-container
        image: my-app:v2  # <-- New version to trigger rolling update
        ports:
        - containerPort: 80
ğŸš€ Steps to Trigger a Rolling Update:
1.Edit the YAML file and change the container image tag (e.g., from v1 to v2).
2.Apply the changes:

bash
kubectl apply -f deployment.yaml

3.Kubernetes will:
-Spin up one new pod (v2) while keeping the others (v1) running.
-Wait for it to become healthy.
-Then terminate one of the old pods.
-Repeat until all old pods are replaced.

âœ… Verify Rollout Progress:
bash
kubectl rollout status deployment my-app

ğŸ”„ Rollback if Needed:
bash
kubectl rollout undo deployment my-app

ğŸ” Why It's Useful:
Rolling updates ensure zero downtime, maintain high availability, and allow safe rollbacks â€” all of which are critical for production-grade systems.


-----------------------------------------------------------------

Whatâ€™s a ConfigMap vs. Secret? How do you use them in k8s deployments?

In Kubernetes, both ConfigMaps and Secrets are used to externalize configuration from application code. This supports separation of concerns, environment-specific customization, and security best practices.

ğŸ”¹ ConfigMap:
-Stores non-sensitive configuration data as key-value pairs.
-Examples: app settings, URLs, port numbers, log levels.
-Stored in plain text (Base64 not required).
-Ideal for parameters that can vary across environments but are not confidential.

Usage Example:
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  LOG_LEVEL: "debug"
  API_ENDPOINT: "https://dev.api.example.com"

ğŸ” Secret:
-Used to store sensitive data such as passwords, API keys, TLS certs.
-Encoded in Base64 and handled more securely by Kubernetes.
-Stored in etcd, optionally encrypted at rest.
-Can be integrated with external secret managers (e.g., AWS Secrets Manager, Vault).

Usage Example:
apiVersion: v1
kind: Secret
metadata:
  name: db-secret
type: Opaque
data:
  DB_USER: dXNlcg==        # base64 for 'user'
  DB_PASS: cGFzc3dvcmQ=    # base64 for 'password'

ğŸ”§ How I Use Them in Deployments:
1.Mount as Environment Variables:
yaml
env:
- name: LOG_LEVEL
  valueFrom:
    configMapKeyRef:
      name: app-config
      key: LOG_LEVEL
- name: DB_USER
  valueFrom:
    secretKeyRef:
      name: db-secret
      key: DB_USER

2.Mount as Volumes (optional for files):
yaml
volumeMounts:
- name: config-volume
  mountPath: /etc/config
volumes:
- name: config-volume
  configMap:
    name: app-config

ğŸ›¡ï¸ Best Practices:
-Use ConfigMaps for non-sensitive configurations that might change across environments.
-Use Secrets for credentials or sensitive tokens â€” and restrict access via RBAC.
-Avoid hardcoding secrets into images or code â€” inject them at runtime via Secrets.
-Use .env to ConfigMap/Secret translators for developer convenience during deployment.

In my experience, Iâ€™ve used ConfigMaps for toggling feature flags and Secrets for managing database credentials, with Helm charts handling environment overrides across dev/stage/prod.

----------------------------------------------------------------------------------------

How do you handle resource limits and requests in Kubernetes?

In Kubernetes, resource requests and limits are used to manage and control how much CPU and memory a container can use. I configure them carefully to ensure efficient resource utilization, application stability, and cluster health.

ğŸ”¹ What They Are:
1.Resource Requests:
-The minimum amount of CPU/memory a container needs to run.
-Used by the scheduler to decide where to place a pod.
-If not enough resources are available on a node, the pod wonâ€™t be scheduled.

2.Resource Limits:
-The maximum amount a container is allowed to consume.
-If a container tries to exceed the limit:
  For CPU: itâ€™s throttled.
  For Memory: itâ€™s killed (OOM - out of memory).

ğŸ”§ How I Set Them (Example YAML):
yaml

resources:
  requests:
    memory: "256Mi"
    cpu: "250m"
  limits:
    memory: "512Mi"
    cpu: "500m"

âœ… My Approach to Managing Resource Requests and Limits:
1.Profiling and Benchmarking:
-I start by profiling the application in staging to understand actual resource usage under load.
-Tools like Prometheus, Kube-Metrics-Server, and Goldilocks help tune these values.

2.Set Reasonable Defaults:
-I define default limits and requests in LimitRanges at the namespace level to avoid runaway pods.

3.Environment-Specific Adjustments:
-Lower limits in dev for cost control.
-Right-sized limits in prod for performance and resilience.

4.Monitor and Tune Continuously:
-Monitor CPU throttling and OOM kills via Grafana dashboards.
-Use Vertical Pod Autoscaler (VPA) to recommend better values over time.

ğŸ“Œ Why It Matters:
-Prevents noisy-neighbor issues.
-Ensures fair resource distribution in multi-tenant clusters.
-Avoids unnecessary overprovisioning (which leads to wasted cloud spend).

In my experience, properly setting resource requests and limits helped us stabilize workloads, reduce cloud costs, and avoid production outages due to memory pressure or CPU starvation.


------------------------------------------------------------------------------

How do you handle rollbacks in k8s?

In Kubernetes, I handle rollbacks through a combination of native deployment strategies, observability, and CI/CD integration. Rollbacks are treated as a critical part of release reliability and business continuity.

âœ… 1. Kubernetes Native Rollback via Deployment History
Kubernetes maintains a revision history of Deployment objects. In case a release introduces issues (e.g., failed probes, broken functionality), I can trigger a rollback using:

bash
kubectl rollout undo deployment <deployment-name>
This reverts to the previous stable replica set version without needing to reapply YAML manually.

I often use:
bash
kubectl rollout history deployment <deployment-name>
to review and validate versions before rolling back.

âœ… 2. Proactive Health Checks with Probes
I implement liveness and readiness probes in all deployments to help Kubernetes detect and isolate bad pods early. If a new deployment fails probes, Kubernetes can auto-revert to the last healthy state (depending on strategy) or at least block traffic from routing to failed pods.

âœ… 3. Canary & Blue-Green Deployments (Helm + ArgoCD/FluxCD)
For high-risk production changes, I prefer canary deployments using Helm + Argo Rollouts or blue-green deployments using service selector switches or ingress annotations.
These allow staged rollouts and instant rollback by promoting the previous version with zero downtime.

âœ… 4. GitOps-Based Rollback via ArgoCD
If I use ArgoCD for GitOps, I can easily roll back to a previous Git commit/tag representing a known good state. ArgoCD syncs the desired state from Git, and rollback is as simple as reverting a commit and triggering a re-sync.

âœ… 5. CI/CD-Driven Rollback Strategy
In Jenkins or GitHub Actions, I include a rollback stage triggered manually or conditionally:
-Based on monitoring tools like Prometheus, ELK, or Datadog alerts.
-Or by parsing failed health checks during post-deployment validation.
-I use Helm charts or kubectl commands programmatically in these CI pipelines to handle rollbacks.

âœ… 6. Observability and Monitoring
Rollbacks are never done blindly. I integrate tools like:
-Prometheus + Grafana for live metrics,
-Loki/ELK for logs, and
-AlertManager or PagerDuty for incident response.

This helps correlate issues with deployments and make informed rollback decisions.

Summary:
treat rollback as a first-class citizen in Kubernetes deployments by using:
-Native rollout history
-Helm/Argo-based strategies
-Health probes
-GitOps version control
-And automated CI/CD workflows
-This ensures rapid recovery from failures with minimal impact to users or systems.


--------------------------------------------------------------------------------------


What is a Kubernetes Operator?

A Kubernetes Operator is a method of extending Kubernetes functionality to manage the entire lifecycle of complex, stateful applications. It leverages custom resources (CRDs) and a controller loop to automate tasks typically performed by a human operatorâ€”such as provisioning, scaling, backup, upgrade, failover, and recovery.

âœ… Key Concepts:
1.Custom Resource Definitions (CRDs):
Operators introduce new object types (like MySQLCluster or KafkaBroker) that are specific to the application being managed.

2.Custom Controllers:
The controller continuously watches the state of these resources and reconciles them with the desired state, similar to how Kubernetes manages Deployments or Pods.

âœ… Why Use Operators?
Native Kubernetes way to manage stateful or complex applications (e.g., databases, messaging systems, custom microservices).
Automates operational tasks such as:
-Self-healing
-Auto-scaling
-Version upgrades
-Configuration drift detection
-Backup and recovery

âœ… Real-World Example:
Instead of manually managing a PostgreSQL instance:
An Operator like CrunchyData PostgreSQL Operator can automatically:
-Create the DB cluster
-Handle replicas and failovers
-Take backups to S3
-Monitor health and restore if necessary
-All through Kubernetes-native YAML configuration.

âœ… Operator Frameworks:
Operators are typically built using:
-Operator SDK (by Red Hat) in Go, Ansible, or Helm
-Kubebuilder for Go-based operators
-Java-based SDKs for JVM workloads

âœ… Summary:
In essence, a Kubernetes Operator is like a DevOps SRE engineer encoded in software, managing the lifecycle of applications the Kubernetes wayâ€”declaratively, reliably, and at scale.


------------------------------------------------------------------------------------------


Q. What are Network Policies in Kubernetes?
Network Policies in Kubernetes are a set of rules that control inbound and outbound traffic at the pod level. They define how groups of pods are allowed to communicate with each other and with external endpoints, functioning similarly to firewall rules within a Kubernetes cluster.

They are implemented using labels, and the policies are enforced by the network plugin (CNI) in useâ€”such as Calico, Cilium, or Weave.

Key points:
-They are namespaced resources.
-Control ingress, egress, or both.
-Applied only to pods with matching labels and only enforced if a compatible CNI plugin is used.
-By default, all traffic is allowed unless a policy is created.

Example use cases:
-Restrict frontend pods from accessing database pods directly.
-Allow egress to external APIs only from specific pods.


--------------------------------------------------------------------------------------

How would you upgrade a Kubernetes cluster?
Upgrading a Kubernetes cluster must be done carefully to ensure zero downtime and service reliability. The exact approach depends on whether it's a managed service (like EKS, GKE, AKS) or a self-managed cluster (like kubeadm or K3s).

âœ… Managed Kubernetes (EKS, AKS, GKE):
Check upgrade compatibility of APIs and workloads.
Upgrade the control plane using the cloud console or CLI.
Upgrade node groups:
-Create new node groups with the new version.
-Drain and cordon old nodes (kubectl drain).
-Recreate or delete old nodes once workloads are rescheduled.
-Upgrade kubectl and CLI tools accordingly.
-Test application functionality and monitor closely.

âœ… Self-Managed Kubernetes (e.g., kubeadm):
Backup etcd and important configs.

Upgrade kubeadm, then:

kubeadm upgrade plan

kubeadm upgrade apply <version> on the control plane node.

Upgrade the kubelet and kubectl versions on each node.

Restart the kubelet and validate the cluster health.

Upgrade worker nodes:
Drain, upgrade, and uncordon each node one at a time.

Run kubectl get nodes to ensure all nodes are at the correct version.

Best Practices:
-Always test upgrades in staging first.
-Use PodDisruptionBudgets (PDBs) to avoid downtime.
-Monitor cluster health post-upgrade using Prometheus/Grafana or Cloud-native tools.


------------------------------------------------------------------------------------------

Q. How would you back up a Kubernetes cluster? What tools would you use?

Backing up a Kubernetes cluster involves safeguarding both the etcd datastore (which contains cluster state) and any persistent application data (like volumes attached to pods). I follow a multi-layered backup strategy to ensure complete recoverability in case of disaster.

ğŸ§  1. Control Plane Backup â€“ etcd
The etcd datastore is the source of truth for the Kubernetes cluster configuration. For backing it up:

1.On self-managed clusters (e.g., kubeadm):
Use the etcdctl snapshot save command:
bash
ETCDCTL_API=3 etcdctl snapshot save /backup/etcd-snapshot.db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key
Automate and schedule snapshots via cron jobs.

2.On managed services (EKS, GKE, AKS):
The control plane is managed by the provider, but I ensure any available automated backups or snapshots are enabled through the cloud provider.

ğŸ“¦ 2. Persistent Volume (PV) Backup
To back up application data stored on PersistentVolumes (e.g., MySQL data, file storage):
I use Velero as a reliable open-source backup and restore tool:
-It supports backing up Kubernetes resources + persistent volumes.
-Can back up to object storage like AWS S3, GCS, or Azure Blob.

Example command:
bash
velero backup create my-backup --include-namespaces my-namespace
Velero also supports scheduled backups and disaster recovery across clusters.

Other tools I've used:
-Kasten K10 (enterprise-grade, especially in large orgs).
-Stash by AppsCode (for more granular backups of volumes).
-Restic for file-level backups.

âš™ï¸ 3. Cluster Resource Definitions (YAML)
I regularly export Kubernetes manifests (Deployments, Services, ConfigMaps, etc.) into a Git repository as part of GitOps practices.

Tools:
kubectl get all --all-namespaces -o yaml > cluster-state.yaml

Or use kustomize/helm for versioned deployments.

This allows for infrastructure-as-code recovery in new clusters.

âœ… Best Practices I Follow:
-Automate scheduled backups using Velero or cron-based scripts.
-Store backups offsite (e.g., S3 bucket with lifecycle policy).
-Encrypt and compress backups.
-Regularly test restore procedures to validate backup integrity.
-Monitor backup status using Prometheus alerts or Veleroâ€™s CLI/status page.


--------------------------------------

ğ—¤. ğ——ğ—²ğ—³ğ—¶ğ—»ğ—² ğ˜ğ—µğ—² ğ—ğ˜‚ğ—¯ğ—²ğ—¿ğ—»ğ—²ğ˜ğ—²ğ˜€ ğ—–ğ—¡ğ—œ (ğ—–ğ—¼ğ—»ğ˜ğ—®ğ—¶ğ—»ğ—²ğ—¿ ğ—¡ğ—²ğ˜ğ˜„ğ—¼ğ—¿ğ—¸ğ—¶ğ—»ğ—´ ğ—œğ—»ğ˜ğ—²ğ—¿ğ—³ğ—®ğ—°ğ—²).
Answer: The Kubernetes CNI is a specification that defines a standardized interface for integrating with container networking plugins, enabling different networking solutions to work with Kubernetes clusters.

ğ—¤. ğ—ªğ—µğ—®ğ˜ ğ—¶ğ˜€ ğ˜ğ—µğ—² ğ—½ğ˜‚ğ—¿ğ—½ğ—¼ğ˜€ğ—² ğ—¼ğ—³ ğ˜ğ—µğ—² ğ—ğ˜‚ğ—¯ğ—²ğ—¿ğ—»ğ—²ğ˜ğ—²ğ˜€ ğ—”ğ—£ğ—œ ğ˜€ğ—²ğ—¿ğ˜ƒğ—²ğ—¿?
Answer: The API server is the front-end interface for the Kubernetes control plane that exposes the Kubernetes API.

ğ—¤. ğ—˜ğ˜…ğ—½ğ—¹ğ—®ğ—¶ğ—» ğ˜ğ—µğ—² ğ—¿ğ—¼ğ—¹ğ—² ğ—¼ğ—³ ğ—²ğ˜ğ—°ğ—± ğ—¶ğ—» ğ—ğ˜‚ğ—¯ğ—²ğ—¿ğ—»ğ—²ğ˜ğ—²ğ˜€.
Answer: etcd is a distributed, reliable, and highly available key-value store used to store the configuration data for the Kubernetes cluster.

ğ—¤. ğ—ªğ—µğ—®ğ˜ ğ—¶ğ˜€ ğ˜ğ—µğ—² ğ—¿ğ—¼ğ—¹ğ—² ğ—¼ğ—³ ğ˜ğ—µğ—² ğ—ğ˜‚ğ—¯ğ—²ğ—¿ğ—»ğ—²ğ˜ğ—²ğ˜€ ğ˜€ğ—°ğ—µğ—²ğ—±ğ˜‚ğ—¹ğ—²ğ—¿?
Answer: The Kubernetes scheduler is responsible for scheduling pods to run on available nodes in the cluster based on available resources and other scheduling requirements.

ğ—¤. ğ——ğ—²ğ˜€ğ—°ğ—¿ğ—¶ğ—¯ğ—² ğ˜ğ—µğ—² ğ—³ğ˜‚ğ—»ğ—°ğ˜ğ—¶ğ—¼ğ—» ğ—¼ğ—³ ğ˜ğ—µğ—² ğ—¸ğ˜‚ğ—¯ğ—²-ğ—°ğ—¼ğ—»ğ˜ğ—¿ğ—¼ğ—¹ğ—¹ğ—²ğ—¿-ğ—ºğ—®ğ—»ğ—®ğ—´ğ—²ğ—¿.
Answer: The kube-controller-manager is responsible for running various controller processes that monitor the state of the cluster and make changes as necessary.

ğ—¤. ğ—ªğ—µğ—®ğ˜ ğ—¶ğ˜€ ğ˜ğ—µğ—² ğ—½ğ˜‚ğ—¿ğ—½ğ—¼ğ˜€ğ—² ğ—¼ğ—³ ğ˜ğ—µğ—² ğ—°ğ—¹ğ—¼ğ˜‚ğ—±-ğ—°ğ—¼ğ—»ğ˜ğ—¿ğ—¼ğ—¹ğ—¹ğ—²ğ—¿-ğ—ºğ—®ğ—»ğ—®ğ—´ğ—²ğ—¿?
Answer: The cloud-controller-manager is responsible for managing integration with cloud providers, such as AWS, GCP, or Azure.

ğ—¤. ğ—˜ğ˜…ğ—½ğ—¹ğ—®ğ—¶ğ—» ğ˜ğ—µğ—² ğ—¿ğ—¼ğ—¹ğ—² ğ—¼ğ—³ ğ˜ğ—µğ—² ğ—¸ğ˜‚ğ—¯ğ—²ğ—¹ğ—²ğ˜ ğ—¶ğ—» ğ—ğ˜‚ğ—¯ğ—²ğ—¿ğ—»ğ—²ğ˜ğ—²ğ˜€.
Answer: The kubelet is an agent that runs on each node and communicates with the Kubernetes API server to manage the container lifecycle.

ğ—¤. ğ—ªğ—µğ—®ğ˜ ğ—¶ğ˜€ ğ˜ğ—µğ—² ğ—³ğ˜‚ğ—»ğ—°ğ˜ğ—¶ğ—¼ğ—» ğ—¼ğ—³ ğ˜ğ—µğ—² ğ—¸ğ˜‚ğ—¯ğ—²-ğ—½ğ—¿ğ—¼ğ˜…ğ˜† ğ—¶ğ—» ğ—ğ˜‚ğ—¯ğ—²ğ—¿ğ—»ğ—²ğ˜ğ—²ğ˜€?
Answer: The kube-proxy is responsible for managing network routing between pods and services in the Kubernetes cluster.

ğ—¤. ğ——ğ—²ğ—³ğ—¶ğ—»ğ—² ğ—® ğ—°ğ—¼ğ—»ğ˜ğ—®ğ—¶ğ—»ğ—²ğ—¿ ğ—¿ğ˜‚ğ—»ğ˜ğ—¶ğ—ºğ—² ğ—¶ğ—» ğ—ğ˜‚ğ—¯ğ—²ğ—¿ğ—»ğ—²ğ˜ğ—²ğ˜€.
Answer: A container runtime is responsible for starting and stopping containers on a node. Examples include Docker, containerd, and CRI-O.

ğ—¤. ğ—ªğ—µğ—®ğ˜ ğ—¶ğ˜€ ğ—® ğ—ğ˜‚ğ—¯ğ—²ğ—¿ğ—»ğ—²ğ˜ğ—²ğ˜€ ğ˜€ğ—²ğ—¿ğ˜ƒğ—¶ğ—°ğ—²?
Answer: A Kubernetes service is an abstraction layer that exposes a set of pods as a network service, allowing them to communicate with each other and with other services outside the cluster.

ğ—¤. ğ—˜ğ˜…ğ—½ğ—¹ğ—®ğ—¶ğ—» ğ—ğ˜‚ğ—¯ğ—²ğ—¿ğ—»ğ—²ğ˜ğ—²ğ˜€ ğ——ğ—¡ğ—¦.
Answer: Kubernetes DNS is a service that provides DNS resolution for services and pods in a Kubernetes cluster, enabling them to discover and communicate with each other using DNS names.

ğ—¤. ğ—ªğ—µğ—®ğ˜ ğ—¶ğ˜€ ğ—® ğ—½ğ—¼ğ—± ğ—»ğ—²ğ˜ğ˜„ğ—¼ğ—¿ğ—¸ ğ—¶ğ—» ğ—ğ˜‚ğ—¯ğ—²ğ—¿ğ—»ğ—²ğ˜ğ—²ğ˜€?
Answer: A pod network is a network overlay that connects pods in a Kubernetes cluster, enabling them to communicate with each other across different nodes.


---------------------------------------

What are liveness and readiness probes in Kubernetes, and why are they important?
ğŸ”¹ Liveness Probe
Definition: Checks if the container is still running/healthy.
Purpose: Detects situations where the container is running but stuck (e.g., deadlock, infinite loop).
Action: If the liveness probe fails, Kubernetes kills and restarts the container.

âœ… Example:

livenessProbe:
  httpGet:
    path: /healthz
    port: 8080
  initialDelaySeconds: 10
  periodSeconds: 5

ğŸ”¹ Readiness Probe
Definition: Checks if the container is ready to serve requests.
Purpose: Prevents traffic from being sent to a pod before itâ€™s ready (e.g., waiting for app to load configs, connect to DB).
Action: If the readiness probe fails, Kubernetes removes the pod from the Service load balancer, but does NOT restart it.

âœ… Example:
readinessProbe:
  httpGet:
    path: /ready
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 5

ğŸ”¹ Why They Are Important
High Availability: Prevents sending traffic to unready pods.
Self-Healing: Liveness ensures unhealthy pods get restarted automatically.
Graceful Deployments: Readiness prevents downtime during rolling updates.
Reliability: Helps distinguish between a pod thatâ€™s alive but unready vs. dead/unrecoverable.

In Kubernetes, liveness and readiness probes are used to monitor the health of applications. A liveness probe checks whether a container is alive â€” if it fails, Kubernetes restarts the container to recover from issues like deadlocks. A readiness probe checks whether the container is ready to serve traffic â€” if it fails, the pod is temporarily removed from the Service endpoints but not restarted.

They are important because they ensure high availability, self-healing, and smooth rollouts. For example, during a deployment, readiness probes prevent traffic from reaching pods that are still initializing, while liveness probes ensure unhealthy pods are automatically replaced.


---------------------------------------

Walk through the steps of deploying an application with Docker and Kubernetes. 

1ï¸âƒ£ Containerize the Application with Docker
Write a Dockerfile (define base image, dependencies, app code, entrypoint).
Build the image:
docker build -t myapp:1.0 .
Test locally:
docker run -p 8080:8080 myapp:1.0
Push to Registry (Docker Hub, ECR, GCR, ACR):
docker tag myapp:1.0 myrepo/myapp:1.0
docker push myrepo/myapp:1.0

2ï¸âƒ£ Prepare Kubernetes Manifests
Deployment (defines desired state of Pods running your Docker image):

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: myrepo/myapp:1.0
        ports:
        - containerPort: 8080


Service (exposes Pods to the network):

apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: LoadBalancer
  selector:
    app: myapp
  ports:
  - port: 80
    targetPort: 8080

3ï¸âƒ£ Deploy to Kubernetes Cluster

Apply manifests:

kubectl apply -f deployment.yaml
kubectl apply -f service.yaml


Check status:

kubectl get pods
kubectl get svc

4ï¸âƒ£ Verify Deployment

Access the application via LoadBalancer/NodePort/Ingress endpoint.

Monitor health:

kubectl describe pod <pod-name>
kubectl logs <pod-name>

5ï¸âƒ£ Scaling & Updates

Scale app:

kubectl scale deployment myapp-deployment --replicas=5


Rolling update:
Update the image in Deployment YAML â†’ kubectl apply -f deployment.yaml.
Kubernetes does zero-downtime rollout automatically.

âœ… Interview-Ready Summary

"The typical workflow to deploy an application with Docker and Kubernetes starts with containerizing the app using a Dockerfile and pushing the image to a registry. Then I define Kubernetes manifests â€” a Deployment for managing Pods and a Service for exposing them. Using kubectl apply, I deploy these to the cluster, verify that the Pods and Services are running, and test the endpoint. Finally, I can scale replicas, roll out new versions, and monitor health using Kubernetes probes and logging. This ensures the app is packaged, deployed, and managed consistently across environments."


"In our setup, every code push triggers Jenkins. The pipeline builds the app, runs tests, scans dependencies, and then builds a Docker image tagged with the commit hash. The image is pushed to AWS ECR.

For deployment, we use Helm charts stored in a GitOps repo. Jenkins updates the values.yaml with the new image tag and runs a Helm upgrade to deploy to Kubernetes. Kubernetes handles rolling updates automatically, and readiness probes ensure no traffic is routed to unready pods.

We also integrated automated smoke tests post-deployment. If they fail, Jenkins triggers a rollback using the previous Helm release. With this approach, we achieved reliable deployments with minimal manual intervention and faster feedback loops."

--------------------------------------

How do you manage secrets securely in container orchestration environments?

1ï¸âƒ£ Use Native Secret Management
Kubernetes Secrets: Store credentials, tokens, API keys.
Mounted as environment variables or files inside Pods.
Example:
kubectl create secret generic db-secret \
  --from-literal=username=admin \
  --from-literal=password=Passw0rd!

Mounted into Deployment YAML.
âš ï¸ By default, K8s secrets are base64 encoded, not encrypted â†’ so best practice is to enable encryption at rest in kube-apiserver.

2ï¸âƒ£ Integrate with External Secret Managers
Use cloud-native services instead of storing sensitive data only in K8s:
-AWS Secrets Manager
-HashiCorp Vault
-Azure Key Vault
-GCP Secret Manager

Sync secrets into K8s using controllers like:
-External Secrets Operator
-Vault Agent Injector
-Sealed Secrets (Bitnami) for GitOps workflows.

3ï¸âƒ£ Follow Least Privilege & Access Controls
Use RBAC to limit which Pods/Namespaces can access which secrets.
Rotate credentials regularly (automated via secret managers).
Audit access using Kubernetes audit logs.

4ï¸âƒ£ Secure in CI/CD Pipelines
Never hardcode secrets in code or Dockerfiles.
Store them in Jenkins credentials, GitHub Actions secrets, GitLab CI variables.
Inject them at runtime rather than committing to repos.

5ï¸âƒ£ Best Practices
Enable etcd encryption (for Kubernetes Secrets at rest).
Network policies to restrict which pods can talk to DB or API endpoints.
Short-lived tokens â†’ prefer IAM roles / service accounts over static creds.
Audit & rotate secrets frequently.

âœ… Interview-Ready Answer

*"I never hardcode secrets in code or container images. In Kubernetes, I use Secrets to inject credentials into pods, but since they are only base64 encoded by default, I enable encryption at rest for etcd. For higher security, I integrate with cloud secret managers like AWS Secrets Manager or HashiCorp Vault, and use tools like External Secrets Operator or Sealed Secrets for GitOps workflows.

I also enforce RBAC so only specific namespaces and service accounts can access certain secrets. In CI/CD pipelines, secrets are injected at runtime from Jenkins or GitHub Actions, never stored in source control. Along with rotation policies and audit logs, this ensures secrets are managed securely across environments."*	

--------------------------------------------

One of your Kubernetes pods is stuck in CrashLoopBackOff. What diagnostic steps would you take?

1. Check Pod Status & Events

Run:
kubectl describe pod <pod-name>

-Look at Events section for image pull errors, config errors, OOMKilled, etc.
-Check if restart count is increasing (confirms crash loop).

2. Check Logs
Get the logs of the failing container:

kubectl logs <pod-name> --previous
--previous shows logs from the last crashed attempt.

Look for errors like missing configs, failed DB connections, unhandled exceptions.

3. Inspect Resource Limits
Sometimes the pod is OOMKilled because memory limits are too low.
Check in kubectl describe pod â†’ Last State: Terminated (OOMKilled).
Adjust CPU/memory requests and limits if needed.

4. Validate Config & Secrets
Ensure required ConfigMaps and Secrets are mounted and environment variables are available.
Check for typos or missing values.

5. Check Health Probes
Misconfigured liveness/readiness probes can cause repeated restarts.
Example: probe hitting the wrong port/path.
Validate probe configs in the Deployment YAML.

6. Check Dependencies
If the pod depends on external services (DB, APIs, message queues), verify connectivity.
Run a temporary debug pod:
kubectl run tmp-shell --rm -i --tty --image=alpine -- sh
to test network/DNS resolution.

7. Check Image & Command
Make sure the container image has the correct entrypoint/command.
Sometimes CrashLoopBackOff happens if the app exits immediately (wrong CMD).

8. Rollback / Redeploy
If the issue is tied to a recent deployment, roll back to the last working version while troubleshooting.

âœ… Example to tell in interview:
"For example, I once had a pod in CrashLoopBackOff due to an invalid database password secret. I used kubectl logs --previous to see the connection error, verified the secret in Kubernetes, fixed it, and redeployed. To prevent this, we added pre-deployment checks for required secrets."

-------------------------------------------
One of your nodes is NotReady for more than 10 minutes. How would you troubleshoot and fix it?

If a node shows NotReady for more than 10 minutes, Iâ€™d start by describing the node with kubectl describe node <node-name> to check conditions like memory pressure, disk pressure, or network issues.

Then Iâ€™d SSH into the node and check the kubelet service (systemctl status kubelet, journalctl -u kubelet) to see if itâ€™s running properly. Iâ€™d also check if the container runtime (Docker/Containerd) is healthy, and verify networking (CNI plugin logs) to ensure the node can talk to the API server.

If the node is truly unhealthy, Iâ€™d cordon and drain it (kubectl drain <node>) so workloads move to healthy nodes, and then either restart services or replace the node if needed.

For prevention, Iâ€™d set up monitoring and alerts on node conditions, and if itâ€™s a cloud-managed cluster, Iâ€™d enable the cluster autoscaler so unhealthy nodes are replaced automatically.â€
-------------------------------------------

one of your node is not ready  for 10 minutes troubleshootin ?

ğŸ”¹ Troubleshooting Steps
1. Check node status & conditions
kubectl get nodes
kubectl describe node <node-name>
 -Look for conditions: Ready, DiskPressure, MemoryPressure, NetworkUnavailable.
 - If status = NotReady, kubelet might not be heartbeating to API server.

2. Check if pods are affected
kubectl get pods -o wide --all-namespaces | grep <node-name>

Confirm if workloads are stuck or rescheduled to other nodes.

3. SSH into the node
Check kubelet and container runtime:

systemctl status kubelet -l
journalctl -u kubelet -f
systemctl status docker   # or containerd

Restart if down:
systemctl restart kubelet
systemctl restart docker   # or containerd

4. Check node resources
Disk full?
df -h

Memory/CPU pressure?
top
free -m

Logs filling up?
du -sh /var/log/*

5. Check CNI / Networking
If node is NotReady due to network:

systemctl status kubelet
systemctl status kube-proxy

Validate that CNI pods (calico, flannel, weave, cilium) are running:

kubectl get pods -n kube-system -o wide

6. Check API server connectivity
From the node:
curl -k https://<api-server>:6443/healthz

If API server unreachable â†’ node isolated (network/firewall issue).

7. Check certificates
Sometimes node kubelet certs expire â†’ re-register needed:
ls -l /var/lib/kubelet/pki/
Rotate certs if expired:
kubeadm certs renew all
systemctl restart kubelet

ğŸ”¹ Possible Causes & Fixes
Cause							Fix
Kubelet stopped					Restart kubelet service
Docker/containerd crashed			Restart container runtime
Disk full (/var/lib/kubelet or /var/log)	Clean old logs, set log rotation
Network plugin crashed (Calico/Flannel)		Restart CNI pods
Node lost connectivity to API server		Check firewall, routes, security groups
Cert expired					Renew kubelet certs, re-join node
High CPU/memory pressure			Increase resources or reschedule workloads

âœ… Interview-style Answer:
"If a node is NotReady for more than 10 minutes, my first step is to check node conditions using kubectl describe node. Then I SSH into the node to check kubelet and container runtime logs. Common issues Iâ€™ve seen are kubelet crash, disk full under /var/lib/kubelet, or network plugin failures. For example, once in production, a node was NotReady due to /var/log filling up, which caused disk pressure. I cleared logs, configured log rotation, restarted kubelet, and the node recovered. I also monitor these conditions in Grafana to get alerts before nodes go down."
-------------------------------------------

How do you roll back or fix this without affecting production?

Scenario A: a node is NotReady â€” fix without user impact
Goal: keep the app serving while we repair the node.

1.Protect traffic first
Cordon the bad node (no new pods scheduled):
kubectl cordon <node>
Check replicas/health of affected workloads. If needed, temporarily scale up other replicas to keep capacity:
kubectl get deploy -A
kubectl scale deploy/<name> -n <ns> --replicas=<N+>

2.Evict safely
Respect PodDisruptionBudget (PDB) while draining to avoid taking too many replicas down:

kubectl get pdb -A
kubectl drain <node> --ignore-daemonsets --delete-emptydir-data

If PDB blocks drain (good!), increase replicas or temporarily relax the PDB, then drain.

3.Repair off-line
SSH to node; fix root cause (kubelet/containerd down, disk pressure, CNI, cert expiry).

Typical quick wins:
# check
systemctl status kubelet containerd || docker
df -h; du -sh /var/log/* /var/lib/kubelet/*
# remediate
systemctl restart containerd kubelet
# if logs/disk:
journalctl --vacuum-time=7d
# if CNI pods crashed, restart daemonset or node

4.Return node to service
Verify Ready:
kubectl get nodes

Uncordon:
kubectl uncordon <node>

Watch pods reschedule and SLOs/metrics (latency/error rate).
Net effect: live traffic stays on healthy nodes; you drain + fix the bad node without customer impact.

Scenario B: a bad application rollout â€” roll back instantly, no downtime

Immediate rollback (Kubernetes Deployment)
Inspect rollout:
-kubectl rollout status deploy/<app> -n <ns>
-kubectl rollout history deploy/<app> -n <ns>

Roll back to last good revision:
-kubectl rollout undo deploy/<app> -n <ns>
# or to a specific revision
-kubectl rollout undo deploy/<app> --to-revision=<rev> -n <ns>

Watch readiness gates before declaring success:
kubectl get pods -w -n <ns>

If you use image pinning
Set image back to previous, record a new revision:
kubectl set image deploy/<app> <container>=<repo>:<previous-tag> -n <ns> --record

Zero-downtime rollout settings (prevent future impact)
Ensure Deployment strategy is safe:
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxSurge: 1
    maxUnavailable: 0

Health checks tuned:
readinessProbe: { httpGet: { path: /healthz, port: 8080 }, initialDelaySeconds: 10, periodSeconds: 5, failureThreshold: 3 }
livenessProbe:  { httpGet: { path: /livez,   port: 8080 }, initialDelaySeconds: 30 }

Keep PDB so rollouts/drains donâ€™t drop too many replicas:

minAvailable: 80%  # or maxUnavailable: 1

Canary/Blue-Green (preferred for risky changes)
Canary: deploy v2 with 5â€“10% traffic (weighted Ingress/Service mesh). Promote or roll back instantly by shifting weights.
Blue-Green: stand up blue (new), keep green (prod). Flip the Service/Ingress only after health passes. Revert is a pointer flip.

CI/CD rollback lever
-Keep immutable artifacts (versioned images). Revert the pipeline to last green build and redeploy.
-If the change was config only, use ConfigMap/Secret versioning and roll back the reference.

Scenario C: Terraform/infra change broke something â€” revert without outage
Donâ€™t patch by hand (avoids drift).
Reapply last good state/artifacts:
Revert Git to last good commit and:

terraform plan -refresh-only
terraform apply

If blast radius is big, targeted apply:
-terraform apply -target=module.app_alb -target=aws_autoscaling_group.app

Use blue/green infra where possible (new ASG/Launch Template), cut traffic via ALB/NLB, then destroy the old stack after validation.
Communication & Guardrails (what interviewers love to hear)
-SLO-driven decision: roll back if error rate/latency breaches for N minutes.
-Feature flags: disable the risky code path without redeploy.
-Runbook: have codified steps (cordon/drain/rollback/validate).
-Observability first: check dashboards (latency, 5xx, saturation), logs, and events before/after action.
-Post-incident: create a fix-forward PR with tests, tighten probes/PDB/strategy, and add pre-deploy checks.


â€œIf a node is unhealthy, I cordon & drain to protect traffic, fix it off-path, then uncordon. If a release is bad, I use kubectl rollout undo (or flip canary/blue-green) with maxUnavailable=0 and solid probes/PDBs so users arenâ€™t impacted. For infra, I revert IaC to last good commit and apply safely. I always validate with metrics before and after.â€


-------------------------------------------
Q: How do you troubleshoot a PersistentVolumeClaim (PVC) stuck in Pending state in Kubernetes?
When a PVC is in Pending state, it usually means Kubernetes couldnâ€™t bind it to a suitable PersistentVolume (PV) or dynamically provision it. I would troubleshoot step by step:

ğŸ” Step 1: Check PVC Status
kubectl get pvc
kubectl describe pvc <pvc-name>
Look at Events section for errors like â€œno persistent volumes available for this claimâ€ or â€œwaiting for a volume to be provisionedâ€.

ğŸ” Step 2: Verify StorageClass
If the PVC specifies a storageClassName, check whether that StorageClass exists:
kubectl get sc
If storageClassName is empty and default storage class is not set, the claim wonâ€™t be provisioned.

ğŸ” Step 3: Check PV Availability (if using Static Provisioning)
kubectl get pv

Ensure there is a PV:
 -In Available state
 -With storageClassName matching the PVC
 -With equal or larger storage size
 -With compatible access modes (e.g., ReadWriteOnce, ReadWriteMany).

ğŸ” Step 4: Check Dynamic Provisioning
If using EBS, EFS, NFS, Ceph, etc., check if the provisioner is working:

kubectl describe sc <sc-name>

Look for provisioner field (e.g., kubernetes.io/aws-ebs, efs.csi.aws.com).

Ensure the CSI driver or in-tree plugin is installed and running:
kubectl get pods -n kube-system | grep csi

ğŸ” Step 5: Node-Level Checks
If using cloud volumes like AWS EBS or GCP PD:
-Ensure the worker nodes have correct IAM roles/permissions to create volumes.
-Check if the requested zone/region in the PVC matches the node availability zone.

ğŸ” Step 6: Common Fixes
-If storageClass missing â†’ assign correct one or set default:

kubectl patch storageclass <sc-name> -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

-If no PV available â†’ create a matching PV manually.
-If CSI driver issue â†’ restart provisioner pods and check logs:
 kubectl logs <provisioner-pod> -n kube-system


âœ… In short:
I would start from PVC â†’ StorageClass â†’ PV availability â†’ CSI driver health â†’ Node/Cloud IAM configuration. Once identified, I can either bind PVC to an existing PV, fix StorageClass, or redeploy the CSI driver.


-------------------------------------------

Q: CPU usage on a node is consistently high, but HPA isnâ€™t scaling pods. How do you debug?

If the node CPU is high but the Horizontal Pod Autoscaler (HPA) is not scaling pods, it usually means HPA is not seeing the metrics correctly or is misconfigured. I would troubleshoot in layers:

ğŸ” Step 1: Verify HPA status
kubectl get hpa
kubectl describe hpa <hpa-name>
Check Current CPU Utilization vs Target CPU Utilization.
If Current is missing or always 0%, metrics are not being reported.

ğŸ” Step 2: Check Metrics Server
HPA relies on metrics-server for CPU/Memory metrics.
Verify metrics-server is running:
kubectl get pods -n kube-system | grep metrics-server

Check logs for errors:
kubectl logs <metrics-server-pod> -n kube-system

If metrics-server canâ€™t reach kubelets (often due to insecure TLS or RBAC), HPA wonâ€™t work.

ğŸ” Step 3: Validate Pod Metrics
kubectl top pods
kubectl top nodes

-If kubectl top works â†’ metrics-server is fine.
-If it errors out â†’ fix metrics-server connectivity.

ğŸ” Step 4: Check Resource Requests/Limits

HPA scales based on CPU requests, not actual usage.
Example: If a pod requests 500m CPU and uses 800m, HPA sees it as 160% usage.
If requests are missing (no CPU requests defined), HPA may not trigger scaling.

Check with:
kubectl describe pod <pod-name> | grep -A3 "Limits"

ğŸ” Step 5: HPA Configuration
Ensure HPA is targeting the correct deployment:

kubectl describe hpa <hpa-name>
Verify min/max replicas are reasonable.
Ensure correct metrics (CPU, memory, custom metrics) are defined.

ğŸ” Step 6: Node vs Pod Issue
High node CPU â‰  High pod CPU utilization.
Sometimes one system pod (like kube-proxy or logging agent) is hogging CPU, not the app pods.
That would not trigger HPA because HPA only reacts to application pod CPU usage, not node CPU.

Check with:
kubectl top pod --all-namespaces | sort -k3 -nr

to see which pod is consuming CPU.

ğŸ” Step 7: Fixes
If metrics-server issue â†’ fix TLS flags (--kubelet-insecure-tls), RBAC, or restart it.
If missing CPU requests â†’ set requests/limits in Deployment spec.
If wrong target resource â†’ update HPA target.
If node pressure only â†’ use Cluster Autoscaler to add nodes, not HPA.

âœ… Summary Answer:
â€œI would first check the HPA metrics with kubectl describe hpa and kubectl top to confirm if CPU utilization is being reported. If not, Iâ€™d troubleshoot metrics-server. If metrics exist but scaling isnâ€™t happening, Iâ€™d check if CPU requests are set correctly in the Deployment, since HPA works on requests not raw usage. Finally, Iâ€™d confirm that the high node CPU isnâ€™t caused by system pods outside HPAâ€™s scope â€” in which case Cluster Autoscaler is the fix, not HPA.â€

-------------------------------------------
Q: How do you drain a node for maintenance without causing downtime in production?

When a node needs maintenance (e.g., kernel patching, AWS EC2 restart, or hardware issue), I would follow these steps to gracefully move workloads without downtime:

ğŸ” Step 1: Mark the Node Unschedulable
     kubectl cordon <node-name>
-This prevents new pods from being scheduled on this node.
-Existing pods keep running.

ğŸ” Step 2: Drain the Node Gracefully
kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data
--ignore-daemonsets â†’ system pods like fluentd, kube-proxy stay running (theyâ€™ll restart on new node automatically).
--delete-emptydir-data â†’ warns that emptyDir data will be lost (so I check if any pods rely on it before proceeding).

Pods managed by Deployments, ReplicaSets, StatefulSets, DaemonSets will be rescheduled on other healthy nodes.

ğŸ” Step 3: Verify Pods Are Rescheduled
kubectl get pods -o wide
-Confirm that workloads from the drained node have been rescheduled to other nodes.
-Use kubectl get events to watch scheduling activity.

ğŸ” Step 4: Perform Node Maintenance
-Patch OS, upgrade kernel, restart EC2 instance, etc.
-Node stays cordoned until Iâ€™m ready.

ğŸ” Step 5: Bring Node Back
kubectl uncordon <node-name>
-Marks the node schedulable again so new pods can be placed.

âš ï¸ Key Considerations to Avoid Downtime
Check Pod Disruption Budgets (PDBs):
-If a PDB is too strict (e.g., maxUnavailable=0), draining may block.
-Iâ€™d adjust PDBs temporarily if needed.
-kubectl get pdb --all-namespaces

Check Node Capacity:
-Ensure the cluster has enough spare capacity.
-If not, use Cluster Autoscaler to add extra nodes first.

StatefulSets / Databases:
-For apps with persistent volumes, drain carefully. Sometimes we do rolling drains one node at a time.

âœ… Summary Answer:
â€œI would cordon the node to stop new scheduling, then drain it with kubectl drain --ignore-daemonsets --delete-emptydir-data to safely evict workloads. Kubernetes reschedules the pods on healthy nodes. I always check PodDisruptionBudgets and cluster capacity to ensure thereâ€™s no downtime. After maintenance, I uncordon the node to bring it back into service.â€

-------------------------------------------
Q: Ingress is not routing traffic. How would you troubleshoot it?

When an Ingress in Kubernetes is not routing traffic, I go through these steps systematically:

ğŸ” Step 1: Check Ingress Resource
kubectl get ingress -n <namespace>
kubectl describe ingress <ingress-name> -n <namespace>
-Verify hostnames, paths, and backend service mapping.
-Look for misconfigured rules or missing host.

Common mistake: host in ingress (myapp.example.com) doesnâ€™t match the actual DNS.

ğŸ” Step 2: Check Ingress Controller
Ensure the Ingress Controller (NGINX, AWS ALB, Traefik, HAProxy) is running:

kubectl get pods -n ingress-nginx
kubectl logs <ingress-controller-pod> -n ingress-nginx

If controller pods are CrashLoopBackOff or logs show config errors â†’ fix deployment.

ğŸ” Step 3: Check Service
kubectl get svc -n <namespace>
kubectl describe svc <service-name>
-Ensure service type is correct (ClusterIP for Ingress).
-Check service ports match what ingress expects.
(e.g., if ingress routes to port 80 but service listens on 8080 â†’ traffic breaks).

ğŸ” Step 4: Check Endpoints
kubectl get endpoints <service-name> -n <namespace>

-If no endpoints â†’ pods are not selected by service labels.
-Mislabeling (app=web vs app=myweb) is a very common issue.

ğŸ” Step 5: Check DNS & External Access
Ensure domain (app.example.com) resolves to ingress LoadBalancer IP / external IP:

nslookup app.example.com
kubectl get svc -n ingress-nginx
-If DNS points elsewhere â†’ ingress never receives traffic.

ğŸ” Step 6: Check TLS/HTTPS
If using HTTPS, verify TLS secret exists and matches cert:

kubectl describe secret <tls-secret> -n <namespace>
-Misconfigured cert = browser errors.

ğŸ” Step 7: Check Network Policies / Firewalls
-If network policies are enabled, they may block ingress controller traffic to pods.
-On cloud (AWS/GCP/Azure), check security groups / firewall rules.

âœ… Summary Answer:
â€œI would first check the Ingress resource for correct host and path, then verify the ingress controller is running and has no errors. Next, Iâ€™d check the backend Service and Endpoints to ensure pods are correctly matched. I also confirm DNS is pointing to the ingress load balancer, TLS secrets are valid, and no NetworkPolicies or firewalls block traffic. Most commonly, it turns out to be a mismatch between Ingress host/service port or missing service endpoints.â€
---------------------------------------------------------------------------------------------------------------------------------

ğŸ” Q: A Pod is stuck in ContainerCreating. How do you troubleshoot?
When a pod is in ContainerCreating, it means Kubernetes has scheduled the pod but is unable to fully start the container. Iâ€™d debug systematically:

1ï¸âƒ£ Check Pod Events
kubectl describe pod <pod-name> -n <namespace>

Look at the Events section at the bottom.
Common issues:
  -Failed to pull image â†’ image not found or no permissions.
  -FailedMount â†’ PVC/PV or ConfigMap/Secret mount issues.
  -NetworkPlugin cni failed â†’ CNI problem.

2ï¸âƒ£ Check Node Status
kubectl get nodes
kubectl describe node <node-name>
If node is NotReady, pod wonâ€™t start.
Disk pressure, memory pressure, or taints could block scheduling.

3ï¸âƒ£ Check Image Pull Issues
kubectl describe pod <pod-name> | grep "Failed to pull image"
If using private repo â†’ check imagePullSecrets.

Verify credentials:
-kubectl get secret <secret-name> -n <namespace> -o yaml

Try pulling image manually on worker node with docker pull / ctr.

4ï¸âƒ£ Check Volume Mounts (PVC/PV)
If pod uses PVC:
kubectl get pvc -n <namespace>
kubectl describe pvc <pvc-name>
If PVC is in Pending â†’ storage class or PV issue.
Check kubectl get storageclass.

5ï¸âƒ£ Check CNI / Networking
If pod stuck due to networking:
kubectl get pods -n kube-system
kubectl logs <cni-pod> -n kube-system
E.g., Calico/Flannel failing to assign IP.

6ï¸âƒ£ Node Resource Constraints
Pod may not start if node lacks CPU/Memory or disk space.

kubectl describe node <node-name>
kubectl top node <node-name>

Look for Insufficient memory/cpu messages.

7ï¸âƒ£ Security Context / Privileged Issues
-If pod requires privileged mode or hostPath, check if PSP/OPA/Gatekeeper is blocking.
-Look for events like Forbidden: no providers available.

âœ… Summary (Interview Style):
â€œIf a pod is stuck in ContainerCreating, I first describe the pod to check events. Most of the time itâ€™s either an image pull error, a volume mount issue, or a CNI problem. Then I verify node health, PVCs, imagePullSecrets, and networking. In production, Iâ€™ve often seen this when a private Docker registry secret wasnâ€™t configured or when PVC was stuck in Pending because no matching storage class existed.â€

-------------------------------------------

Q: A pod is stuck in CrashLoopBackOff. What diagnostic steps do you take?
When a pod is in CrashLoopBackOff, it means the container inside keeps failing and restarting. I would troubleshoot step by step:

ğŸ” Step 1: Check Pod Status
kubectl get pod <pod-name> -n <namespace>
kubectl describe pod <pod-name> -n <namespace>
-Look at events in describe output â†’ they often show errors like â€œImagePullBackOffâ€, â€œOOMKilledâ€, or â€œCrashLoopBackOffâ€.
-Check restart count to see how often itâ€™s failing.

ğŸ” Step 2: Check Pod Logs
kubectl logs <pod-name> -n <namespace>
kubectl logs <pod-name> -c <container-name> -n <namespace>
-See why the application is crashing.
-Example errors: missing config, DB connection refused, invalid environment variable.
-If itâ€™s restarting too fast, add --previous to check last run logs.

ğŸ” Step 3: Check Container Command / Entrypoint
Sometimes the container exits because the entrypoint script finishes or has a wrong command.

kubectl describe pod <pod-name>
-Look under command: or args: to confirm.

ğŸ” Step 4: Check Resource Limits
kubectl describe pod <pod-name> | grep -A5 "Limits"
kubectl top pod <pod-name> -n <namespace>
-If OOMKilled â†’ container is exceeding memory limits.
-If CPU throttled heavily, it may fail to start.

ğŸ” Step 5: Check ConfigMaps / Secrets
If app needs environment variables or mounted configs:

kubectl describe configmap <cm-name>
kubectl describe secret <secret-name>
-Missing config values or wrong DB credentials â†’ app crash.

ğŸ” Step 6: Debug with kubectl exec (if container runs briefly)
If pod comes up for a few seconds, try to exec inside:
kubectl exec -it <pod-name> -- /bin/sh
Helps check logs, configs, and connectivity inside container.

ğŸ” Step 7: Node-Level Issues
If multiple pods are crashing on the same node â†’ check node resources and kubelet logs:
kubectl describe node <node-name>
kubectl get events --sort-by=.metadata.creationTimestamp

âœ… Summary (Interview Style):
â€œI would start with kubectl describe pod to see events, then check the logs of the container to identify why itâ€™s failing. Next, Iâ€™d confirm if the crash is due to bad entrypoint commands, missing configs, or resource issues like OOMKilled. If needed, Iâ€™d verify ConfigMaps, Secrets, and Service dependencies. In production, most of the CrashLoopBackOff issues Iâ€™ve seen were due to wrong environment variables, DB not reachable, or resource limits being too strict.â€


-------------------------------------------

kubernetes advanced scenario based questions production grade?

1. etcd is running out of space in production â€“ how do you handle it?
First, check etcd health:
ETCDCTL_API=3 etcdctl --endpoints=<etcd-endpoint> endpoint health

Compact etcd DB if too many old keys:
ETCDCTL_API=3 etcdctl compact <rev-number>

Defrag to reclaim disk:
ETCDCTL_API=3 etcdctl defrag

For long-term â†’ set TTLs on non-critical keys, monitor disk, run etcd in a dedicated node group with persistent storage.

2. Suddenly all pods in a namespace are in ImagePullBackOff. How do you debug?
-Check if the container registry is down.
-Verify imagePullSecrets:

kubectl get secret <secret-name> -n <namespace> -o yaml

Look for expired registry credentials (common in AWS ECR, DockerHub rate-limiting).
Fix â†’ update the secret with a fresh token and restart pods.
In production, set up image caching (Harbor, Artifactory, ECR cache).

3. You deployed a new version, but 10% of traffic is failing. How do you roll back safely?
Check rollout status:
kubectl rollout status deployment <deployment>

Identify failing pods with kubectl logs.

Rollback immediately:
kubectl rollout undo deployment <deployment>

For production â†’ use Canary deployments with Argo Rollouts/Istio so only a small % of traffic goes to new version first.

4. A node went down in production. Some critical pods didnâ€™t reschedule. Why?
-Pod has nodeSelector/affinity tied to that node.
-Pod uses local PV (hostPath, local storage) â†’ not portable.
-Pod is a DaemonSet â†’ runs only on specific nodes.
Solution â†’
-Use StorageClasses + dynamic provisioning instead of local PV.
-Add podAntiAffinity and PDBs for HA.
-Enable Cluster Autoscaler for node replacement.

5. Your cluster is running out of resources during peak load. How do you handle autoscaling?
-Enable Horizontal Pod Autoscaler (HPA) for scaling pods based on CPU/memory/custom metrics.
-Use Cluster Autoscaler to add/remove worker nodes dynamically.
-Use Vertical Pod Autoscaler (VPA) to adjust pod requests/limits.
-In production â†’ always combine HPA + Cluster Autoscaler + PodDisruptionBudgets for resiliency.

6. You notice API server latency is very high. How do you debug?
-Check API server logs.
-Inspect etcd health (slow etcd = slow API).
-Look at request volume â€“ maybe too many clients or controllers spamming.
-Use metrics from kube-apiserver (via Prometheus).
-Apply request limits (API rate limiting) for aggressive clients (e.g., CI/CD pipelines).

7. How do you handle secrets in production securely?
-Never store secrets in plain Kubernetes Secrets (theyâ€™re just base64).
-Use Sealed Secrets or External Secrets Operator with AWS Secrets Manager/HashiCorp Vault.
-Enable Encryption at Rest for secrets in etcd.
-Restrict RBAC â€“ only specific service accounts can access secrets.

8. A namespace is consuming too many resources and affecting others. How do you prevent this?
-Use ResourceQuotas to limit total CPU/Memory per namespace.
-Use LimitRanges to enforce per-pod resource limits.
-In multi-tenant clusters, use OPA/Gatekeeper or Kyverno to enforce policies cluster-wide.

9. Pods are stuck in Terminating state forever. How do you resolve this?
-Pod has a Finalizer that didnâ€™t execute â†’ remove it manually:
-kubectl patch pod <pod> -p '{"metadata":{"finalizers":[]}}' --type=merge
-Stuck due to unresponsive NFS/PV â†’ force delete pod:
-kubectl delete pod <pod> --grace-period=0 --force
-Long-term â†’ fix storage driver / cleanup finalizers in controller.

10. You need to perform cluster upgrades in production. How do you ensure zero downtime?
-Upgrade using Managed K8s (EKS/GKE/AKS) where control plane upgrades are automated.
-Use PodDisruptionBudgets (PDBs) to prevent evicting too many pods at once.
-Upgrade worker nodes gradually using node draining:
-kubectl drain <node> --ignore-daemonsets --delete-emptydir-data
-Use RollingUpdate strategy so deployments reschedule pods without downtime.
-Test in staging before production.

--------------------------------------------------------------------------------------------------------------------------------

Q. How do you ensure zero downtime deployment in Kubernetes?

Ensuring zero downtime deployment in Kubernetes requires careful handling of deployments, rolling updates, health checks, and traffic management. Hereâ€™s my approach:
1ï¸âƒ£ Use Rolling Updates
Kubernetes Deployment supports rolling updates by default.
Example in a deployment YAML:
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0

maxUnavailable: 0 ensures no pod is terminated until a new pod is ready, guaranteeing uptime.
maxSurge: 1 allows one extra pod to be scheduled temporarily to handle traffic.

2ï¸âƒ£ Configure Readiness and Liveness Probes
Readiness probes ensure a pod doesnâ€™t receive traffic until it is fully ready.
readinessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 10
  periodSeconds: 5

Liveness probes ensure failed pods are restarted automatically without affecting healthy traffic.

3ï¸âƒ£ Use Service + Load Balancer
Kubernetes Service routes traffic only to ready pods.
During deployment, the old pods serve traffic until new pods pass readiness checks, ensuring continuous availability.

4ï¸âƒ£ Optional: Blue-Green / Canary Deployment
For critical applications:
Blue-Green: Deploy new version in separate pods, switch traffic when verified.
Canary: Gradually route a small percentage of traffic to new pods, monitor metrics, then shift 100%.

5ï¸âƒ£ Monitor Metrics During Deployment
Use Prometheus/Grafana to monitor CPU, memory, error rates, response time.
Rollback immediately if SLA breaches or error rates increase.

6ï¸âƒ£ Rollback Strategy
With deployments:
kubectl rollout undo deployment <deployment-name>
This allows instant rollback to last stable version without downtime.

âœ… Key Interview Points:
-Rolling updates with maxUnavailable: 0.
-Readiness/liveness probes to control traffic.
-Blue-Green / Canary for critical releases.
-Continuous monitoring and fast rollback.

-------------------------------------------


1. Node is NotReady for 10+ minutes
â€œIâ€™d start by checking node status using kubectl get nodes and describe it with kubectl describe node <node-name> to see taints, conditions, or kubelet issues. Then Iâ€™d SSH into the node and check kubelet logs (journalctl -u kubelet) and system metrics (CPU, disk pressure, memory). If the node is unhealthy, Iâ€™d cordon and drain it, then either restart kubelet or replace the node. For long-term, Iâ€™d ensure monitoring on node health and use cluster autoscaler for replacement automation.â€

2. Pods stuck in Pending even though nodes are free
â€œIâ€™d check kubectl describe pod to see why scheduling failed â€” usually resource requests donâ€™t fit, missing nodeSelector/tolerations, or PVC not bound. Iâ€™d also check cluster quotas and taints. If itâ€™s a scheduling constraint, Iâ€™d fix node labels or adjust requests/limits. If itâ€™s PVC-related, Iâ€™d verify the storage class and binding. Monitoring scheduling events helps prevent recurrence.â€

3. HPA not scaling pods under high CPU load
â€œFirst, Iâ€™d check if metrics-server is running (kubectl top pods) since HPA depends on it. Then Iâ€™d describe the HPA (kubectl describe hpa) to see why it didnâ€™t trigger scaling. Sometimes pods donâ€™t have proper CPU requests defined, so utilization never triggers. Iâ€™d fix by setting resource requests/limits and validating metrics. If needed, Iâ€™d scale nodes with Cluster Autoscaler to support more pods.â€

4. Pod stuck in CrashLoopBackOff
â€œIâ€™d check logs with kubectl logs <pod> -p to see the last crash. Common causes are misconfigured env vars, DB connection failures, or app bugs. Iâ€™d also check kubectl describe pod for events like OOMKilled or readiness probe failures. If itâ€™s config-related, Iâ€™d fix ConfigMap/Secret. If itâ€™s resource-related, Iâ€™d adjust limits. For resilience, Iâ€™d add retries and health checks in the app.â€

5. Deployment succeeds but traffic still goes to old pods
â€œIâ€™d check the Service definition to ensure the selector matches the new pod labels. If old pods are still endpoints, the Service might be misconfigured. Iâ€™d also check Ingress or DNS caching. Running kubectl get endpoints helps confirm routing. To prevent this, Iâ€™d use canary/rolling updates with proper readiness probes so only healthy new pods receive traffic.â€

6. Pods getting OOMKilled
â€œIâ€™d check pod events and logs (kubectl describe pod) â€” OOMKilled means the pod exceeded its memory limit. Iâ€™d analyze app memory usage with metrics (kubectl top pod) and adjust requests/limits appropriately. If itâ€™s a memory leak, Iâ€™d involve devs to fix. For prevention, Iâ€™d set resource quotas and alerts on pod restarts.â€

7. Pod cannot reach external DB/API
â€œIâ€™d first exec into the pod and test connectivity (curl, ping, nslookup). If DNS resolution fails, Iâ€™d check CoreDNS logs. If network fails, Iâ€™d check NetworkPolicies, security groups (in cloud), or firewall rules. Also, verify if the cluster uses a NAT Gateway for outbound internet. For long-term, Iâ€™d maintain network policies and document external dependencies.â€

8. Service of type LoadBalancer not accessible
â€œIâ€™d check if the cloud provider provisioned the LB (kubectl get svc). Then verify external IP is assigned and firewall/security group rules allow inbound. Iâ€™d also check if target pods are ready and part of the Endpoints. If itâ€™s still failing, Iâ€™d test directly with NodePort to isolate LB issues. Prevention: monitoring LB provisioning events and readiness checks.â€

9. PVC stuck in Pending (PV not bound)
â€œIâ€™d describe the PVC to see why binding failed. Often the storage class is missing or no matching PV exists. Iâ€™d check kubectl get sc and provision a PV with matching size and access mode. If dynamic provisioning is enabled, Iâ€™d check cloud storage permissions. To prevent this, Iâ€™d standardize default StorageClasses and quotas.â€

10. etcd corruption / cluster recovery
â€œIf etcd is corrupted, the control plane is impacted. Iâ€™d first check if we have an etcd snapshot backup (etcdctl snapshot restore). In managed clusters (EKS/GKE/AKS), Iâ€™d rely on cloud-managed control plane recovery. For self-managed clusters, Iâ€™d restore from the latest snapshot and restart the API server. As a best practice, Iâ€™d schedule automated etcd backups and test DR playbooks.â€


-------------------------------------------
Q. How do you perform Canary Deployment in Kubernetes?

Canary deployment in Kubernetes means gradually rolling out a new version of an application to a small percentage of users, testing its stability, and then promoting it to full traffic if everything is healthy. This ensures we minimize risk compared to deploying to 100% at once."

1ï¸âƒ£ Implementation Approaches in Kubernetes
Option A: Multiple Deployments + Weighted Services
Create two Deployments:
 -Stable (v1) â€“ the current version.
 -Canary (v2) â€“ the new version.

-Expose both behind the same Service.
-Control traffic distribution by adjusting replica counts.

Example:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-v1
spec:
  replicas: 4
  template:
    spec:
      containers:
      - name: app
        image: myapp:v1

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-v2
spec:
  replicas: 1   # Canary gets less traffic
  template:
    spec:
      containers:
      - name: app
        image: myapp:v2


-If Service has 5 pods total, ~20% of traffic goes to canary.
-Gradually increase replicas for canary and decrease stable replicas if monitoring is good.

Option B: Using Ingress / Service Mesh (Advanced)
-With NGINX Ingress Controller, Istio, or Linkerd, we can do weighted routing.
-Example with Istio VirtualService:

apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: app
spec:
  hosts:
  - myapp.example.com
  http:
  - route:
    - destination:
        host: app-v1
      weight: 90
    - destination:
        host: app-v2
      weight: 10

-Here, only 10% of traffic is sent to canary version.
-This gives finer control than replica-based method.

2ï¸âƒ£ Monitoring Canary
-Use Prometheus + Grafana or ELK for:
 -Error rate (5xx, 4xx)
 -Latency
 -CPU/memory usage
-If metrics are healthy, proceed to increase traffic share.
-If issues appear â†’ rollback to stable.

3ï¸âƒ£ Rollback Strategy
-Delete or scale down the canary Deployment to 0.
-In service mesh approach, change routing weight back to 100% stable.
-Always keep blue/green rollback option ready.

âœ… Key Points Interviewers Expect in Your Answer:
-Canary = small controlled rollout.
-Implemented via replica ratio or traffic splitting (Ingress/Service Mesh).
-Monitoring is critical before full rollout.
-Always have a rollback path.

-------------------------------------------
Q. How do you set resource limits in Kubernetes?

In Kubernetes, I set resource requests and limits in the Pod or Deployment manifest under the resources field. This ensures fair scheduling and prevents any single container from consuming excessive cluster resources."

1ï¸âƒ£ Example YAML
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: my-app-container
        image: my-app:latest
        resources:
          requests:
            cpu: "500m"    # Minimum guaranteed CPU (0.5 core)
            memory: "256Mi" # Minimum guaranteed memory
          limits:
            cpu: "1"       # Maximum 1 core
            memory: "512Mi" # Maximum 512MB memory

2ï¸âƒ£ How It Works
-Requests: Minimum resources Kubernetes guarantees. Scheduler uses this to decide where the Pod can run.
-Limits: Maximum resources a container can use.
 -If CPU > limit â†’ throttling occurs.
 -If Memory > limit â†’ container gets OOMKilled.

3ï¸âƒ£ Production Practices
-Namespace ResourceQuota & LimitRange:
  -I enforce quotas at namespace level so teams donâ€™t consume all cluster resources.
  -Example: set default limits for every Pod in a namespace.

-Monitoring:
 -I monitor actual usage with Prometheus + Grafana.
 -Adjust requests/limits based on real utilization â†’ avoids over-provisioning and throttling.

Best Practices:
 -Start with actual usage data (kubectl top pods) before setting.
 -Avoid setting requests too low (can cause eviction).
 -Avoid setting limits too high (can starve other apps).

âœ… Key Takeaway for Interviewer:
"I always define CPU/memory requests and limits in manifests. I also enforce namespace-level quotas and monitor usage. This prevents noisy-neighbor problems and ensures predictable performance in production."

-------------------------------------------

How do you monitor logs in Kubernetes?

"In Kubernetes, logs are critical for troubleshooting and monitoring workloads. I approach it in multiple layers depending on whether itâ€™s basic debugging or centralized production monitoring."

1ï¸âƒ£ Pod-Level (On-the-fly Debugging)
Use kubectl logs to view logs from a Pod:

kubectl logs <pod-name>
kubectl logs <pod-name> -c <container-name>     # if multiple containers
kubectl logs -f <pod-name>                            # stream logs
kubectl logs --previous <pod-name>                 # check logs from a crashed container

For multi-pod deployments:
kubectl logs -l app=my-app --all-containers
This fetches logs from all Pods with a specific label.

2ï¸âƒ£ Centralized Logging (Production Standard)
Since Pods are ephemeral, logs must be aggregated. I typically use EFK or ELK stack or cloud-native logging:
EFK (Elasticsearch + Fluentd/Fluent Bit + Kibana)
 -Fluent Bit/Fluentd collects container logs from nodes.
 -Elasticsearch stores & indexes logs.
 -Kibana provides dashboards and search.
Promtail + Loki + Grafana
 -Loki is lightweight, optimized for K8s.
 -Cheaper and integrates seamlessly with Grafana.
Cloud Provider Solutions:
 -AWS EKS â†’ CloudWatch Container Insights.
 -GCP GKE â†’ Cloud Logging.
 -Azure AKS â†’ Azure Monitor.

3ï¸âƒ£ Monitoring Practices
-Structured Logging: Enforce JSON log format for easier parsing.
-Correlation IDs: Add request IDs in microservices to trace transactions across logs.
-Log Retention & Cost Control: Define log retention policies (e.g., 30 days for prod, 7 days for dev).
-Alerts on Logs: Integrate with Prometheus + Alertmanager or cloud alerting to trigger alerts based on error log patterns.

4ï¸âƒ£ Example Production Setup
-"In my last project, we deployed Fluent Bit as a DaemonSet to ship logs from all nodes to Elasticsearch. Then, we visualized and searched logs in Kibana. We also had log-based alerts integrated with PagerDuty for critical errors like OutOfMemory or repeated 5xx responses."

âœ… Key Takeaway for Interviewer:
"I use kubectl logs for quick debugging, but in production, I always set up centralized log aggregation using EFK/Loki or cloud-native tools, enforce structured logging, and integrate alerts to ensure issues are detected early."


-------------------------------------------
What is Ingress and Ingress controller

ğŸ”¹ Ingress
Ingress is a Kubernetes API object that manages external access to services in a cluster.
It provides HTTP and HTTPS routing to internal services.
Instead of exposing each service with a LoadBalancer or NodePort, you define centralized routing rules in Ingress.

ğŸ‘‰ Example use cases:
Route app.company.com/login â†’ login-service
Route app.company.com/api â†’ api-service
Terminate SSL/TLS at the edge instead of inside pods.

Ingress = Rules (WHAT traffic should go where).

ğŸ”¹ Ingress Controller
Ingress itself is just a specification â€” it needs a controller to make it work.
Ingress Controller is the actual implementation that watches Ingress resources and configures a reverse proxy / load balancer accordingly.
Popular controllers:
 -NGINX Ingress Controller
 -AWS ALB Ingress Controller (AWS Load Balancer Controller)
 -HAProxy
 -Traefik

Ingress Controller = Implementation (HOW traffic is handled).

ğŸ”¹ Analogy
Ingress = A set of traffic rules (like road signs: "cars to the left, trucks to the right").
Ingress Controller = The traffic police who enforce those rules (actually directing the traffic).

ğŸ”¹ Example: Ingress Resource
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
spec:
  rules:
  - host: app.company.com
    http:
      paths:
      - path: /api
        pathType: Prefix
        backend:
          service:
            name: api-service
            port:
              number: 80
      - path: /login
        pathType: Prefix
        backend:
          service:
            name: login-service
            port:
              number: 80

Here:
/api traffic goes to api-service
/login traffic goes to login-service

âœ… Interview-Ready Answer
"Ingress is a Kubernetes object that defines routing rules for HTTP/S traffic into the cluster, while an Ingress Controller is the component that implements those rules by configuring a reverse proxy/load balancer like NGINX or ALB. Without an Ingress Controller, Ingress objects donâ€™t do anything. For example, Iâ€™ve used the AWS ALB Ingress Controller in EKS to expose microservices securely with SSL termination and path-based routing."

-------------------------------------------

What Happens if the Scheduler Fails?

ğŸ”¹ What Happens if the Scheduler Fails?
-The cluster keeps running existing workloads (already scheduled pods are unaffected).
-But new pods cannot be scheduled (theyâ€™ll remain in Pending state).
-Auto-scaling, rolling updates, and rescheduling of failed pods wonâ€™t happen.

ğŸ”¹ How to Handle Scheduler Failures
1. Check Scheduler Health
Run:
kubectl get pods -n kube-system | grep scheduler

Logs:
kubectl logs -n kube-system kube-scheduler-<node-name>

2. If Scheduler Pod Crashed
-If using static pod (default in kubeadm setups), it runs as a pod defined in /etc/kubernetes/manifests/kube-scheduler.yaml.
-Check if the file is present and valid.
-Restart kubelet to re-create it:
-systemctl restart kubelet

3. If Scheduler is Stuck or Misconfigured
-Validate its config (/etc/kubernetes/scheduler.conf).
-Common issues: wrong kubeconfig, API Server connectivity issues, certificate expiration.
-Fix and redeploy.

4. Failover / HA Setup
-In production, always run multiple scheduler instances for HA.
-Only one scheduler is active at a time (leader election).
-If one fails, another takes over automatically.
-Ensure you deployed Kubernetes in HA mode (multi-master setup).

5. Temporary Manual Scheduling (Workaround)
-If urgent, you can manually assign pods to nodes by setting nodeName in Pod spec:
spec:
  nodeName: worker-1

Or use kubectl patch to bind a pod:
kubectl patch pod mypod -p '{"spec":{"nodeName":"worker-1"}}'

âœ… Interview-Ready Answer
"If the scheduler fails, existing workloads continue running but new pods remain in Pending state. My first step is to check the scheduler pod status and logs in the kube-system namespace. If itâ€™s a static pod issue, I fix its manifest under /etc/kubernetes/manifests/ and restart kubelet. If itâ€™s a config or connectivity issue, I validate the scheduler config and API server access. In production, we mitigate this by running multiple schedulers with leader election so another instance takes over automatically. As a temporary workaround, pods can also be scheduled manually by assigning them to specific nodes."

-------------------------------------------



-------------------------------------------


-------------------------------------------



-------------------------------------------


-------------------------------------------



-------------------------------------------







